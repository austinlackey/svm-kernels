\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pifont}
\usepackage{fontawesome5}
\usepackage{tikz-3dplot}
\usepackage{forest}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}

\title{Methodology of Creating SVM Kernels from Scratch Using Python and NumPy}
\author{Austin Lackey}
\author{Tomy Sabalo Farias}
\affil{DSCI 320, Colorado State University}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a methodology for creating Support Vector Machine (SVM) kernels from scratch using Python and NumPy. 
We discuss the implementation of linear, sigmoid, polynomial, and radial basis function (RBF) kernels in a binary and multiclass SVM.
These kernels are tested on E. coli data and compared to the results of the scikit-learn SVM implementation.
The results show that the implemented kernels often yield better accuracy than the scikit-learn implementation; at the
cost of increased training time. Kernel training times are ran multiple times and averaged to provide a more accurate
metric since training times are low and have a high variance.
\end{abstract}

\section{Introduction}
In this section, provide an introduction to SVMs, their applications, and the importance of
kernels in SVMs.

Support Vector Machines (SVMs) are machine learning algorithms that are used to complete classification tasks. When we have a large dimensional dataset that we would like to create decision boundaries for, we can create a hyperplane that maximizes the margin between the observations' labels. The data points that are closest to the decision boundary influence the position and direction of the hyperplane, and these close data points are called \textbf{Support Vectors}. The goal is to optimize the distance between the support vectors and the hyperplane, and so we can draw a line between the points.

With higher dimensional data, sometimes a linear boundary will not be effective, and so we can implement kernel tricks. These kernels allow for the original input to be mapped to a higher-dimensional space where a linear boundary can be drawn. We will demonstrate the usage of SVMs, as well as the effect that different kernels have on the same data. 

\section{Methodology}
In this section, describe the methodology used to create the SVM kernels from scratch.

When in discussion of SVMs, we need to understand the steps behind the math to know what it is doing. Here, we will discuss how this algorithm is run to calculate our optimal parameters for our hyperplane decision boundary.

Our goal is to find a hyperplane in the form:
$$f(x) = w \cdot x + b = 0$$

We want our decision boundary to have a maximal margin between the hyperplane and the support vectors. These vectors are found with:
$$w \cdot x + b \geq 1$$
$$w \cdot x + b  \leq -1$$
Since we want to maximize the margin in between, we can think about finding those inputs that satisfy the support vectors in the right labels. For example, if we find an $x_n$ such that it is the $n$th observation in the dataset, and it has the label $y_n = 1$, then we want this observation to be found above the positive support vector. Mathematically, we can say that:
$$\begin{cases}
      w \cdot x + b \geq 1 & \text{if } y = 1 \\
      w \cdot x + b \leq -1 & \text{if } y = -1
\end{cases}$$
In other words, we can simply this to be:

$$y_i (w \cdot x_i + b) \geq 1$$ for $i = 1,...,N$

The support vectors are described, but we want to find the maximum length a unit step takes from the decision boundary to the support vectors to be optimized. Therefore, if we are given a vector $x$, with an orthogonal vector $w$ to the decision boundary, we can take a unit step from the decision boundary to the support vector by:
$$1 = w \cdot x + b = w(x + \frac{w}{|w|^2}) + b = 1$$

We know that $w \cdot x + b = 0$, so we can simplify:
$$\frac{ww}{|w|^2} = 1$$
$$\frac{1}{|w|}$$
This represents half the margin of the SVM, and so we need to do it to both sides of the boundary:
$$\frac{2}{|w|}$$
Therefore, to maximize the margin of the hyperplane, we need to minimize $w$.

We can combine this all to formulate our optimization problem: minimize $||w||$ constrained by $y_i (w \cdot x_i + b) \geq 1$ for $i = 1,...,N$
We can set up a Lagrange dual optimization problem, such that:
$$L = \frac{1}{2}||w||^2 - \sum_{i=1}^{n} \alpha_i \cdot [y_i(w \cdot x + b) - 1]$$
Minimizing for $w$ and $b$, we get:
$$w = \sum_{i=1}^{n} \alpha_i y_i x_i$$
$$\sum_{i=1}^{n} \alpha_i y_i = 0$$

Our problem 
\subsection{Kernel Functions}
We implemented four different kernel functions which are listed below.
\begin{itemize}
    \item Linear Kernel
    \begin{equation}
    K(X, Y) = X^T Y
    \end{equation}
    
    \item Sigmoid Kernel
    \begin{equation}
    K(X, Y) = \tanh(\gamma X^T Y + r)
    \end{equation}
    
    \item Polynomial Kernel
    \begin{equation}
    K(X, Y) = (\gamma X^T Y + r)^d, \gamma > 0
    \end{equation}
    
    \item Radial Basis Function (RBF) Kernel
    \begin{equation}
    K(X, Y) = \exp(-\gamma ||X - Y||^2), \gamma > 0
    \end{equation}
\end{itemize}
\subsection{Binary SVM}
Discuss the implementation of the Binary SVM class, including the implementation of the different kernels.



\subsection{Multiclass SVM}
Discuss the implementation of the Multiclass SVM class, which uses the Binary SVM class.

\section{Results and Discussion}
In this section, present and discuss the results obtained using the implemented SVM kernels.
\begin{table}[h]
    \centering
    \caption{Classifier Performance}
    \label{tab:classifier_performance}
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Classifier} & \textbf{Implementation} & \textbf{Kernel} & \textbf{Avg Accuracy} & \textbf{Avg Runtime} \\
        \midrule
        Binary & sklearn & linear & 1.00000 & 0.00052 \\
        Binary & custom & linear & 0.99318 & 0.00795 \\
        Binary & sklearn & sigmoid & 0.68180 & 0.00066 \\
        Binary & custom & sigmoid & 0.99546 & 0.00757 \\
        Binary & sklearn & rbf & 1.00000 & 0.00054 \\
        Binary & custom & rbf & 0.99546 & 0.00977 \\
        Binary & sklearn & poly & 1.00000 & 0.00040 \\
        Binary & custom & poly & 0.97498 & 0.00929 \\
        Multi & sklearn & linear & 0.77940 & 0.00096 \\
        Multi & custom & linear & 0.81468 & 0.15420 \\
        Multi & sklearn & sigmoid & 0.47060 & 0.00183 \\
        Multi & custom & sigmoid & 0.82497 & 0.15641 \\
        Multi & sklearn & rbf & 0.75000 & 0.00156 \\
        Multi & custom & rbf & 0.82350 & 0.19177 \\
        Multi & sklearn & poly & 0.76470 & 0.00111 \\
        Multi & custom & poly & 0.83967 & 0.36326 \\
        \bottomrule
    \end{tabular}
    \vspace{1em}  % Add some vertical space for the note
    \begin{tabular}{p{0.9\textwidth}}  % Create a new table for the note
        \multicolumn{1}{l}{\textit{Note:}} These times were calculated with 10 runs for each kernel and averaged. \\
    \end{tabular}
\end{table}
\section{Conclusion}
In this section, provide a conclusion summarizing the work done and its implications.

% THESE ARE EXAMPLES OF HOW TO USE BIBTEX
% \begin{thebibliography}{9}
% \bibitem{numpy}
% Travis E, Oliphant. 
% A guide to NumPy, USA: Trelgol Publishing, (2006).

% \bibitem{python}
% Van Rossum, G., Drake, F.L. 
% Python 3 Reference Manual, Scotts Valley, CA: CreateSpace, (2009).

% \bibitem{svm}
% Cortes, C., Vapnik, V. 
% Support-vector networks. Machine Learning, 20(3):273-297, (1995).
% \end{thebibliography}

\end{document}